{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive MLP to detect spam\n",
    "Here: build the most basic solution that classifies emails.\n",
    "We don't expect a good solution, just something that works so that we can implement changes later on, one at a time.\n",
    "\n",
    "Naive ideas :\n",
    "Word embedder : naive solution which consists of going through all the words in the file and assigning them the order in which they are encountered.\n",
    "Classifier itself : MLP\n",
    "\n",
    "It is also necessary to clean the data (transform to lowercase ...) but I describe that in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd # conda install -c anaconda pandas\n",
    "\n",
    "#Deep learning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Text processing\n",
    "import tensorflow as tf # conda install -c conda-forge tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "A tokenizer converts a string, such as a sentence, into individual tokens. These may be words or numbers. The simplest tokenizer consists in separating a sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here’s',\n",
       " 'to',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'ones,',\n",
       " 'the',\n",
       " 'misfits,',\n",
       " 'the',\n",
       " 'rebels,',\n",
       " 'the',\n",
       " 'troublemakers']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Here’s to the crazy ones, the misfits, the rebels, the troublemakers\"\n",
    "text.split() # python method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several details to take into account. For example, whether to include punctuation and how to handle upper case letters. The tokenizer method from tensorflow.keras allows us to transform text into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5, 1, 6, 1, 7, 1, 8]]\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow.keras method\n",
    "text2 = [\"Here’s to the crazy ones, the misfits, the rebels, the troublemakers\"]\n",
    "        # LIST containing our string, not the same thing I don't know why\n",
    "tokenizer = Tokenizer(num_words=50, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(text2) \n",
    "sequences = tokenizer.texts_to_sequences(text2)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine each command.\n",
    "\n",
    "*fit_on_texts* updates the vocabulary of our tokenizer. Each word is assigned an index based on how frequent it appears in our text. So the most common word gets assigned 1, the second most common 2 and so on. The index 0 is reserved for padding.\n",
    "\n",
    "We can define a limit to the size of our vocabulary, defined in the first line with *num_word=50*, which can allow us to filter out rare words. The tokenizer method filters out all punctuation by default, and *lower=True* converts all uppercase characters to lowercase. Setting *char_level=False* simply means we split at the level of words rather than at the level of characters.\n",
    "\n",
    "*texts_to_sequences* transforms the given text into a sequence of indices, using the internal vocabulary of our tokenizer. Having the two commands be separate allows us to \"train\" our vocabulary on one text and convert any new text we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'here’s': 2,\n",
       " 'to': 3,\n",
       " 'crazy': 4,\n",
       " 'ones': 5,\n",
       " 'misfits': 6,\n",
       " 'rebels': 7,\n",
       " 'troublemakers': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Following the tutorial on https://towardsdatascience.com/nlp-spam-detection-in-sms-text-data-using-deep-learning-b8632db85cc8 to get something started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ThinhNguyendai/SMSSpamDetection/main/SMSSpamCollection\" #Use the RAW one\n",
    "messages = pd.read_csv(url, sep ='\\t', names=[\"label\", \"message\"])\n",
    "messages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label                                            message\n",
      "103   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "154   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "207   ham  As I entered my cabin my PA said, '' Happy B'd...\n",
      "223   ham                             Sorry, I'll call later\n",
      "326   ham                   No calls..messages..missed calls\n",
      "339   ham                             Sorry, I'll call later\n",
      "357  spam  Congratulations ur awarded 500 of CD vouchers ...\n",
      "444   ham                             Sorry, I'll call later\n",
      "533   ham                  Gudnite....tc...practice going on\n",
      "655   ham                       Did u got that persons story\n"
     ]
    }
   ],
   "source": [
    "duplicatedRow = messages[messages.duplicated()]\n",
    "print(duplicatedRow[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5572 messages, 4825 of which are ham and 747 are spam. The dataset is **highly unbalanced, so we need to do something about it.**. We also have 403 duplicate messages.\n",
    "\n",
    "There are many ways to handle unbalanced dataset, and it is worth exploring other ways than the one shown in the link. The author uses downsampling, which simply deletes observations from the class that is overrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_msg = messages[messages.label =='ham']\n",
    "spam_msg = messages[messages.label=='spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747, 2) (747, 2)\n"
     ]
    }
   ],
   "source": [
    "ham_msg_df = ham_msg.sample(n = len(spam_msg), random_state = 704)\n",
    "spam_msg_df = spam_msg\n",
    "print(ham_msg_df.shape, spam_msg_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Maybe?! Say hi to  and find out if  got his ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Jos ask if u wana meet up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>I dont know oh. Hopefully this month.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dear Hero,i am leaving to qatar tonite for an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>No that just means you have a fat head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>spam</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>spam</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Maybe?! Say hi to  and find out if  got his ca...\n",
       "1      ham                         Jos ask if u wana meet up?\n",
       "2      ham              I dont know oh. Hopefully this month.\n",
       "3      ham  Dear Hero,i am leaving to qatar tonite for an ...\n",
       "4      ham             No that just means you have a fat head\n",
       "...    ...                                                ...\n",
       "1489  spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "1490  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "1491  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "1492  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "1493  spam  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df = pd.concat([ham_msg_df, spam_msg_df])\n",
    "msg_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_labels = (msg_df['label'].map({'ham': 0, 'spam': 1})).values\n",
    "            # map creates a dataframe where we replace values\n",
    "            # .values is to extract the values as an array\n",
    "msg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_msg, test_msg, train_labels, test_labels = train_test_split(msg_df['message'],\n",
    "                                                                  msg_labels,\n",
    "                                                                  test_size=0.2,\n",
    "                                                                  random_state=705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5039    Thanks for being there for me just to talk to ...\n",
      "1122    Do you want 750 anytime any network mins 150 t...\n",
      "1751                           Got it..mail panren paru..\n",
      "1376                              We're finally ready fyi\n",
      "3906    Do you want a new video handset? 750 anytime a...\n",
      "                              ...                        \n",
      "3562    Text BANNEDUK to 89555 to see! cost 150p texto...\n",
      "5537    Want explicit SEX in 30 secs? Ring 02073162414...\n",
      "1952    Haha... Really oh no... How? Then will they de...\n",
      "4432       2mro i am not coming to gym machan. Goodnight.\n",
      "2808    December only! Had your mobile 11mths+? You ar...\n",
      "Name: message, Length: 1195, dtype: object\n",
      "\n",
      "\n",
      "[0 1 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_msg)\n",
    "print(\"\\n\")\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer : turn words into integers\n",
    "oov_tok = \"<OOV>\" # What to replace words that are not in the vocabulary with\n",
    "vocab_size = 500 # Maximum number of words for tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size,\n",
    "                      char_level=False, # Work words by word\n",
    "                      oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'to': 2,\n",
       " 'you': 3,\n",
       " 'a': 4,\n",
       " 'i': 5,\n",
       " 'call': 6,\n",
       " 'the': 7,\n",
       " 'your': 8,\n",
       " 'u': 9,\n",
       " 'for': 10,\n",
       " 'is': 11,\n",
       " 'now': 12,\n",
       " 'and': 13,\n",
       " '2': 14,\n",
       " 'free': 15,\n",
       " 'or': 16,\n",
       " 'on': 17,\n",
       " 'in': 18,\n",
       " 'have': 19,\n",
       " 'ur': 20,\n",
       " 'txt': 21,\n",
       " 'me': 22,\n",
       " 'from': 23,\n",
       " '4': 24,\n",
       " 'of': 25,\n",
       " 'are': 26,\n",
       " 'text': 27,\n",
       " 'with': 28,\n",
       " 'it': 29,\n",
       " 'this': 30,\n",
       " 'get': 31,\n",
       " 'mobile': 32,\n",
       " 'just': 33,\n",
       " 'stop': 34,\n",
       " 'no': 35,\n",
       " 'my': 36,\n",
       " 'only': 37,\n",
       " 'reply': 38,\n",
       " 'claim': 39,\n",
       " 'will': 40,\n",
       " 'send': 41,\n",
       " 'out': 42,\n",
       " 'so': 43,\n",
       " 'if': 44,\n",
       " 'that': 45,\n",
       " 'be': 46,\n",
       " 'we': 47,\n",
       " 'our': 48,\n",
       " 'prize': 49,\n",
       " 'www': 50,\n",
       " 'can': 51,\n",
       " 'do': 52,\n",
       " 'not': 53,\n",
       " 'at': 54,\n",
       " 'cash': 55,\n",
       " 'but': 56,\n",
       " 'won': 57,\n",
       " '150p': 58,\n",
       " 'new': 59,\n",
       " 't': 60,\n",
       " 'win': 61,\n",
       " 'msg': 62,\n",
       " '1': 63,\n",
       " 'please': 64,\n",
       " 'phone': 65,\n",
       " '50': 66,\n",
       " 'who': 67,\n",
       " 'uk': 68,\n",
       " 'week': 69,\n",
       " 'urgent': 70,\n",
       " 'go': 71,\n",
       " 'all': 72,\n",
       " 'nokia': 73,\n",
       " 'tone': 74,\n",
       " \"i'm\": 75,\n",
       " 'service': 76,\n",
       " 'when': 77,\n",
       " 'min': 78,\n",
       " 'what': 79,\n",
       " 'know': 80,\n",
       " 'want': 81,\n",
       " 'r': 82,\n",
       " 'by': 83,\n",
       " 'c': 84,\n",
       " 'up': 85,\n",
       " 'been': 86,\n",
       " 'good': 87,\n",
       " 'back': 88,\n",
       " 'contact': 89,\n",
       " 'how': 90,\n",
       " '16': 91,\n",
       " '18': 92,\n",
       " 'customer': 93,\n",
       " 'co': 94,\n",
       " 'com': 95,\n",
       " 'message': 96,\n",
       " 'per': 97,\n",
       " '£1': 98,\n",
       " 'lt': 99,\n",
       " 'gt': 100,\n",
       " 'then': 101,\n",
       " 'time': 102,\n",
       " 'guaranteed': 103,\n",
       " 'chat': 104,\n",
       " 'day': 105,\n",
       " 'its': 106,\n",
       " 'as': 107,\n",
       " 'today': 108,\n",
       " 'got': 109,\n",
       " 'see': 110,\n",
       " 'ok': 111,\n",
       " 'hi': 112,\n",
       " 'find': 113,\n",
       " '3': 114,\n",
       " 'n': 115,\n",
       " 'he': 116,\n",
       " 'every': 117,\n",
       " '£1000': 118,\n",
       " 'cs': 119,\n",
       " 'was': 120,\n",
       " 'am': 121,\n",
       " 'more': 122,\n",
       " 'love': 123,\n",
       " 'like': 124,\n",
       " 'any': 125,\n",
       " 'mins': 126,\n",
       " 'ringtone': 127,\n",
       " 'holiday': 128,\n",
       " 'has': 129,\n",
       " 'an': 130,\n",
       " 'come': 131,\n",
       " 'camera': 132,\n",
       " 'there': 133,\n",
       " 'receive': 134,\n",
       " 'draw': 135,\n",
       " 'k': 136,\n",
       " 'yes': 137,\n",
       " 'sms': 138,\n",
       " 'line': 139,\n",
       " 'live': 140,\n",
       " 'right': 141,\n",
       " 'going': 142,\n",
       " 'awarded': 143,\n",
       " 'latest': 144,\n",
       " 'one': 145,\n",
       " 'video': 146,\n",
       " 'dont': 147,\n",
       " 'they': 148,\n",
       " 'apply': 149,\n",
       " \"don't\": 150,\n",
       " 'wk': 151,\n",
       " 'about': 152,\n",
       " 'number': 153,\n",
       " 'd': 154,\n",
       " 'ü': 155,\n",
       " 'tomorrow': 156,\n",
       " 'rate': 157,\n",
       " 'tell': 158,\n",
       " 'landline': 159,\n",
       " 'code': 160,\n",
       " 'take': 161,\n",
       " 'box': 162,\n",
       " 'night': 163,\n",
       " 'da': 164,\n",
       " 'network': 165,\n",
       " 'her': 166,\n",
       " '150ppm': 167,\n",
       " 'next': 168,\n",
       " 'pls': 169,\n",
       " 'award': 170,\n",
       " 'chance': 171,\n",
       " 'shows': 172,\n",
       " 'think': 173,\n",
       " 's': 174,\n",
       " 'offer': 175,\n",
       " '000': 176,\n",
       " 'word': 177,\n",
       " 'had': 178,\n",
       " 'orange': 179,\n",
       " 'make': 180,\n",
       " 'entry': 181,\n",
       " 'po': 182,\n",
       " 'need': 183,\n",
       " 'weekly': 184,\n",
       " 'tones': 185,\n",
       " 'over': 186,\n",
       " '£100': 187,\n",
       " 'special': 188,\n",
       " '1st': 189,\n",
       " 'selected': 190,\n",
       " '5': 191,\n",
       " \"i'll\": 192,\n",
       " 'some': 193,\n",
       " 'give': 194,\n",
       " 'collection': 195,\n",
       " '£5000': 196,\n",
       " 'here': 197,\n",
       " 'thanks': 198,\n",
       " 'collect': 199,\n",
       " 'name': 200,\n",
       " 'play': 201,\n",
       " 'she': 202,\n",
       " 'later': 203,\n",
       " 'work': 204,\n",
       " '10p': 205,\n",
       " 'attempt': 206,\n",
       " 'lor': 207,\n",
       " \"it's\": 208,\n",
       " 'update': 209,\n",
       " 'mob': 210,\n",
       " 'poly': 211,\n",
       " 'sae': 212,\n",
       " '500': 213,\n",
       " 'last': 214,\n",
       " 'sorry': 215,\n",
       " 'valid': 216,\n",
       " 'account': 217,\n",
       " 'help': 218,\n",
       " 'national': 219,\n",
       " 'dear': 220,\n",
       " 'delivery': 221,\n",
       " 'yours': 222,\n",
       " '8007': 223,\n",
       " 'bonus': 224,\n",
       " 'real': 225,\n",
       " 'him': 226,\n",
       " '10': 227,\n",
       " 'opt': 228,\n",
       " '2nd': 229,\n",
       " 'why': 230,\n",
       " 'e': 231,\n",
       " 'b': 232,\n",
       " 'music': 233,\n",
       " 'club': 234,\n",
       " \"c's\": 235,\n",
       " 'colour': 236,\n",
       " 'well': 237,\n",
       " 'home': 238,\n",
       " 'top': 239,\n",
       " 'great': 240,\n",
       " 'calls': 241,\n",
       " 'still': 242,\n",
       " 'join': 243,\n",
       " 'await': 244,\n",
       " 'http': 245,\n",
       " 'cant': 246,\n",
       " '£500': 247,\n",
       " 'meet': 248,\n",
       " 'very': 249,\n",
       " '750': 250,\n",
       " 'half': 251,\n",
       " 'v': 252,\n",
       " 'keep': 253,\n",
       " 'vouchers': 254,\n",
       " '86688': 255,\n",
       " 'hot': 256,\n",
       " 'after': 257,\n",
       " 'cost': 258,\n",
       " 'first': 259,\n",
       " 'oh': 260,\n",
       " 'dating': 261,\n",
       " 'texts': 262,\n",
       " 'games': 263,\n",
       " 'should': 264,\n",
       " 'wat': 265,\n",
       " 'pounds': 266,\n",
       " 'price': 267,\n",
       " 'miss': 268,\n",
       " 'quiz': 269,\n",
       " 'which': 270,\n",
       " 'us': 271,\n",
       " 'someone': 272,\n",
       " 'life': 273,\n",
       " 'yeah': 274,\n",
       " 'waiting': 275,\n",
       " 'g': 276,\n",
       " 'pick': 277,\n",
       " 'auction': 278,\n",
       " 'row': 279,\n",
       " 'wanna': 280,\n",
       " '6': 281,\n",
       " 'too': 282,\n",
       " 'best': 283,\n",
       " 'hey': 284,\n",
       " 'sent': 285,\n",
       " 'did': 286,\n",
       " 'trying': 287,\n",
       " 'were': 288,\n",
       " 'say': 289,\n",
       " 'pic': 290,\n",
       " '£3': 291,\n",
       " 'private': 292,\n",
       " 'todays': 293,\n",
       " 'x': 294,\n",
       " '£2000': 295,\n",
       " 'order': 296,\n",
       " 'xmas': 297,\n",
       " 'end': 298,\n",
       " 'hello': 299,\n",
       " 'messages': 300,\n",
       " 'ltd': 301,\n",
       " 'gift': 302,\n",
       " 'tried': 303,\n",
       " 'services': 304,\n",
       " 'bt': 305,\n",
       " 'days': 306,\n",
       " 'anytime': 307,\n",
       " '08000930705': 308,\n",
       " 'final': 309,\n",
       " 'winner': 310,\n",
       " 'operator': 311,\n",
       " '7': 312,\n",
       " 'xxx': 313,\n",
       " 'even': 314,\n",
       " 'sexy': 315,\n",
       " 'm': 316,\n",
       " 'expires': 317,\n",
       " 'land': 318,\n",
       " 'could': 319,\n",
       " 'hope': 320,\n",
       " 'savamob': 321,\n",
       " 'content': 322,\n",
       " '£2': 323,\n",
       " 'buy': 324,\n",
       " 'already': 325,\n",
       " 'suite342': 326,\n",
       " '2lands': 327,\n",
       " 'heart': 328,\n",
       " \"i've\": 329,\n",
       " 'try': 330,\n",
       " 're': 331,\n",
       " '03': 332,\n",
       " 'off': 333,\n",
       " 'freemsg': 334,\n",
       " 'being': 335,\n",
       " 'talk': 336,\n",
       " 'ready': 337,\n",
       " 'camcorder': 338,\n",
       " 'congrats': 339,\n",
       " 'looking': 340,\n",
       " 'reveal': 341,\n",
       " 'pobox': 342,\n",
       " 'enjoy': 343,\n",
       " '12hrs': 344,\n",
       " 'his': 345,\n",
       " 'statement': 346,\n",
       " 'points': 347,\n",
       " 'identifier': 348,\n",
       " 'amp': 349,\n",
       " 'net': 350,\n",
       " 'between': 351,\n",
       " 'tc': 352,\n",
       " 'happy': 353,\n",
       " 'dogging': 354,\n",
       " 'worth': 355,\n",
       " 'offers': 356,\n",
       " 'way': 357,\n",
       " '0800': 358,\n",
       " 'where': 359,\n",
       " 'f': 360,\n",
       " 'part': 361,\n",
       " 'weekend': 362,\n",
       " 'wait': 363,\n",
       " 'sleep': 364,\n",
       " 'eg': 365,\n",
       " 'use': 366,\n",
       " 'caller': 367,\n",
       " 'charged': 368,\n",
       " 'either': 369,\n",
       " 'leave': 370,\n",
       " 'unsubscribe': 371,\n",
       " 'many': 372,\n",
       " 'rental': 373,\n",
       " 'before': 374,\n",
       " 'wkly': 375,\n",
       " 'question': 376,\n",
       " 'std': 377,\n",
       " 'mates': 378,\n",
       " 'speak': 379,\n",
       " 'info': 380,\n",
       " '£250': 381,\n",
       " 'shopping': 382,\n",
       " 'friends': 383,\n",
       " 'babe': 384,\n",
       " 'details': 385,\n",
       " 'representative': 386,\n",
       " 'ever': 387,\n",
       " 'baby': 388,\n",
       " 'girl': 389,\n",
       " 'tv': 390,\n",
       " 'complimentary': 391,\n",
       " 'place': 392,\n",
       " 'welcome': 393,\n",
       " 'important': 394,\n",
       " 'hg': 395,\n",
       " 'voucher': 396,\n",
       " 'wap': 397,\n",
       " 'money': 398,\n",
       " 'age': 399,\n",
       " 'much': 400,\n",
       " '08000839402': 401,\n",
       " 'date': 402,\n",
       " 'ac': 403,\n",
       " '£350': 404,\n",
       " 'double': 405,\n",
       " 'fun': 406,\n",
       " 'having': 407,\n",
       " 'secret': 408,\n",
       " 'admirer': 409,\n",
       " 'thinks': 410,\n",
       " 'comp': 411,\n",
       " 'may': 412,\n",
       " 'flag': 413,\n",
       " 'news': 414,\n",
       " 'answer': 415,\n",
       " 'easy': 416,\n",
       " 'late': 417,\n",
       " 'player': 418,\n",
       " 'pics': 419,\n",
       " 'direct': 420,\n",
       " 'care': 421,\n",
       " 'month': 422,\n",
       " 'mobiles': 423,\n",
       " 'haha': 424,\n",
       " \"there's\": 425,\n",
       " 'cos': 426,\n",
       " '00': 427,\n",
       " 'sub': 428,\n",
       " 'okay': 429,\n",
       " 'del': 430,\n",
       " 'pm': 431,\n",
       " 'choose': 432,\n",
       " 'txts': 433,\n",
       " 'msgs': 434,\n",
       " 'wish': 435,\n",
       " 'phones': 436,\n",
       " 'other': 437,\n",
       " 'coming': 438,\n",
       " 'fancy': 439,\n",
       " 'sex': 440,\n",
       " 'them': 441,\n",
       " 'let': 442,\n",
       " 'ipod': 443,\n",
       " 'congratulations': 444,\n",
       " '11': 445,\n",
       " 'balance': 446,\n",
       " 'mobileupd8': 447,\n",
       " 'im': 448,\n",
       " '£200': 449,\n",
       " 'yo': 450,\n",
       " 'morning': 451,\n",
       " '100': 452,\n",
       " 'told': 453,\n",
       " 'o': 454,\n",
       " 'charity': 455,\n",
       " '150': 456,\n",
       " 'pass': 457,\n",
       " 'england': 458,\n",
       " 'inc': 459,\n",
       " 'custcare': 460,\n",
       " 'rates': 461,\n",
       " 'something': 462,\n",
       " 'minutes': 463,\n",
       " \"you're\": 464,\n",
       " 'found': 465,\n",
       " '2003': 466,\n",
       " '800': 467,\n",
       " '04': 468,\n",
       " 'man': 469,\n",
       " 'charge': 470,\n",
       " 'really': 471,\n",
       " '2004': 472,\n",
       " 'because': 473,\n",
       " 'kiss': 474,\n",
       " 'ask': 475,\n",
       " '£150': 476,\n",
       " 'discount': 477,\n",
       " 'valued': 478,\n",
       " 'guess': 479,\n",
       " 'done': 480,\n",
       " 'reward': 481,\n",
       " 'lucky': 482,\n",
       " '20p': 483,\n",
       " 'ntt': 484,\n",
       " 'tonight': 485,\n",
       " 'hear': 486,\n",
       " 'years': 487,\n",
       " 'gr8': 488,\n",
       " 'address': 489,\n",
       " 'getting': 490,\n",
       " 'start': 491,\n",
       " 'reach': 492,\n",
       " 'wid': 493,\n",
       " 'currently': 494,\n",
       " 'w1j6hl': 495,\n",
       " 'call2optout': 496,\n",
       " 'within': 497,\n",
       " 'nice': 498,\n",
       " '87066': 499,\n",
       " 'numbers': 500,\n",
       " 'eve': 501,\n",
       " 'sure': 502,\n",
       " 'friend': 503,\n",
       " 'polys': 504,\n",
       " 'away': 505,\n",
       " 'rcvd': 506,\n",
       " 'saturday': 507,\n",
       " 'etc': 508,\n",
       " 'bx420': 509,\n",
       " 'ip4': 510,\n",
       " '5we': 511,\n",
       " 'maybe': 512,\n",
       " 'problem': 513,\n",
       " 'txting': 514,\n",
       " 'game': 515,\n",
       " 'specially': 516,\n",
       " 'ill': 517,\n",
       " 'true': 518,\n",
       " 'hmv': 519,\n",
       " 'weeks': 520,\n",
       " 'anyway': 521,\n",
       " 'sweet': 522,\n",
       " 'sony': 523,\n",
       " 'dvd': 524,\n",
       " '82277': 525,\n",
       " 'sp': 526,\n",
       " '50p': 527,\n",
       " 'into': 528,\n",
       " 'feel': 529,\n",
       " 'always': 530,\n",
       " 'card': 531,\n",
       " 'st': 532,\n",
       " 'un': 533,\n",
       " 'redeemed': 534,\n",
       " '£800': 535,\n",
       " 'lol': 536,\n",
       " 'better': 537,\n",
       " 'yr': 538,\n",
       " 'must': 539,\n",
       " 'biz': 540,\n",
       " 'book': 541,\n",
       " 'people': 542,\n",
       " 'guys': 543,\n",
       " 'bored': 544,\n",
       " 'leh': 545,\n",
       " 'dat': 546,\n",
       " 'soon': 547,\n",
       " 'costa': 548,\n",
       " 'sol': 549,\n",
       " 'sk38xh': 550,\n",
       " \"uk's\": 551,\n",
       " '£10': 552,\n",
       " 'summer': 553,\n",
       " 'credit': 554,\n",
       " 'bid': 555,\n",
       " 'terms': 556,\n",
       " 'log': 557,\n",
       " '0870': 558,\n",
       " 'unlimited': 559,\n",
       " 'shit': 560,\n",
       " 'freephone': 561,\n",
       " 'said': 562,\n",
       " 'went': 563,\n",
       " 'ans': 564,\n",
       " 'loyalty': 565,\n",
       " 'meeting': 566,\n",
       " 'wont': 567,\n",
       " 'would': 568,\n",
       " 'food': 569,\n",
       " 'thought': 570,\n",
       " '02': 571,\n",
       " '06': 572,\n",
       " 'luv': 573,\n",
       " 'ending': 574,\n",
       " 'doing': 575,\n",
       " 'thats': 576,\n",
       " 'age16': 577,\n",
       " 'calling': 578,\n",
       " 'song': 579,\n",
       " 'each': 580,\n",
       " '11mths': 581,\n",
       " 'again': 582,\n",
       " '20': 583,\n",
       " 'motorola': 584,\n",
       " 'gay': 585,\n",
       " '08712460324': 586,\n",
       " '87077': 587,\n",
       " 'big': 588,\n",
       " 'handset': 589,\n",
       " 'year': 590,\n",
       " 'getzed': 591,\n",
       " 'flights': 592,\n",
       " 'spree': 593,\n",
       " 'shop': 594,\n",
       " \"t's\": 595,\n",
       " 'things': 596,\n",
       " 'called': 597,\n",
       " 'sunshine': 598,\n",
       " 'ringtones': 599,\n",
       " 'eat': 600,\n",
       " 'while': 601,\n",
       " 'left': 602,\n",
       " 'also': 603,\n",
       " 'weekends': 604,\n",
       " 'tot': 605,\n",
       " 'awesome': 606,\n",
       " 'boy': 607,\n",
       " 'standard': 608,\n",
       " 'forget': 609,\n",
       " '10am': 610,\n",
       " 'txtauction': 611,\n",
       " '£900': 612,\n",
       " 'yesterday': 613,\n",
       " 'area': 614,\n",
       " 'abt': 615,\n",
       " '85023': 616,\n",
       " 'unsub': 617,\n",
       " '09050090044': 618,\n",
       " 'toclaim': 619,\n",
       " 'pobox334': 620,\n",
       " 'stockport': 621,\n",
       " 'cost£1': 622,\n",
       " 'max10mins': 623,\n",
       " 'cheap': 624,\n",
       " 'company': 625,\n",
       " 'o2': 626,\n",
       " \"you've\": 627,\n",
       " 'world': 628,\n",
       " 'visit': 629,\n",
       " 'information': 630,\n",
       " 'user': 631,\n",
       " 'onto': 632,\n",
       " 'fantastic': 633,\n",
       " '1327': 634,\n",
       " 'croydon': 635,\n",
       " 'cr9': 636,\n",
       " '5wb': 637,\n",
       " 'entitled': 638,\n",
       " 'remove': 639,\n",
       " 'stuff': 640,\n",
       " 'minute': 641,\n",
       " 'bus': 642,\n",
       " 'takes': 643,\n",
       " 'entered': 644,\n",
       " 'person': 645,\n",
       " 'might': 646,\n",
       " 'crazy': 647,\n",
       " 'ts': 648,\n",
       " 'arrive': 649,\n",
       " 'maximize': 650,\n",
       " 'digital': 651,\n",
       " '28': 652,\n",
       " 'red': 653,\n",
       " 'working': 654,\n",
       " 'sky': 655,\n",
       " 'ldew': 656,\n",
       " 'down': 657,\n",
       " 'match': 658,\n",
       " '08712300220': 659,\n",
       " '08718720201': 660,\n",
       " 'll': 661,\n",
       " 'princess': 662,\n",
       " 'these': 663,\n",
       " 'store': 664,\n",
       " 'nothing': 665,\n",
       " 'rply': 666,\n",
       " 'sat': 667,\n",
       " 'enter': 668,\n",
       " 'never': 669,\n",
       " 'ldn': 670,\n",
       " 'b4': 671,\n",
       " 'local': 672,\n",
       " 'another': 673,\n",
       " 'long': 674,\n",
       " 'early': 675,\n",
       " 'logo': 676,\n",
       " 'ring': 677,\n",
       " 'mail': 678,\n",
       " 'finally': 679,\n",
       " '150pm': 680,\n",
       " 'woke': 681,\n",
       " 'cup': 682,\n",
       " 'official': 683,\n",
       " 'aight': 684,\n",
       " 'tenerife': 685,\n",
       " 'questions': 686,\n",
       " 'p': 687,\n",
       " 'few': 688,\n",
       " 'job': 689,\n",
       " 'didnt': 690,\n",
       " 'valentines': 691,\n",
       " 'hair': 692,\n",
       " 'liao': 693,\n",
       " 'asked': 694,\n",
       " 'fine': 695,\n",
       " 'pobox84': 696,\n",
       " 'fuck': 697,\n",
       " 'click': 698,\n",
       " 'awaiting': 699,\n",
       " 'wow': 700,\n",
       " 'sea': 701,\n",
       " 'yourself': 702,\n",
       " 'once': 703,\n",
       " 'watching': 704,\n",
       " 'driving': 705,\n",
       " 'member': 706,\n",
       " 'trip': 707,\n",
       " 'dis': 708,\n",
       " 'room': 709,\n",
       " 'hard': 710,\n",
       " 'computer': 711,\n",
       " \"won't\": 712,\n",
       " 'mind': 713,\n",
       " '80488': 714,\n",
       " 'knw': 715,\n",
       " '12': 716,\n",
       " 'hours': 717,\n",
       " 'does': 718,\n",
       " 'locations': 719,\n",
       " 'ec2a': 720,\n",
       " 'urawinner': 721,\n",
       " 'surprise': 722,\n",
       " 'break': 723,\n",
       " 'okie': 724,\n",
       " 'fantasies': 725,\n",
       " '08707509020': 726,\n",
       " 'records': 727,\n",
       " '86021': 728,\n",
       " 'access': 729,\n",
       " 'yet': 730,\n",
       " 'wants': 731,\n",
       " 'correct': 732,\n",
       " 'press': 733,\n",
       " 'ends': 734,\n",
       " 'contacted': 735,\n",
       " 'cum': 736,\n",
       " 'look': 737,\n",
       " 'cancel': 738,\n",
       " 'test': 739,\n",
       " 'forgot': 740,\n",
       " 'tmr': 741,\n",
       " 'oso': 742,\n",
       " 'asap': 743,\n",
       " 'months': 744,\n",
       " \"how's\": 745,\n",
       " '3g': 746,\n",
       " 'videophones': 747,\n",
       " 'videochat': 748,\n",
       " 'java': 749,\n",
       " 'dload': 750,\n",
       " 'noline': 751,\n",
       " 'rentl': 752,\n",
       " 'reference': 753,\n",
       " 'cc': 754,\n",
       " 'words': 755,\n",
       " 'pound': 756,\n",
       " 'cd': 757,\n",
       " 'id': 758,\n",
       " 'voda': 759,\n",
       " 'quoting': 760,\n",
       " 'den': 761,\n",
       " 'decimal': 762,\n",
       " 'wife': 763,\n",
       " 'sir': 764,\n",
       " 'actually': 765,\n",
       " 'though': 766,\n",
       " 'around': 767,\n",
       " 'show': 768,\n",
       " 'frnd': 769,\n",
       " '62468': 770,\n",
       " '25p': 771,\n",
       " '3030': 772,\n",
       " 'till': 773,\n",
       " 'missed': 774,\n",
       " 'plz': 775,\n",
       " 'bluetooth': 776,\n",
       " 'made': 777,\n",
       " 'pobox36504w45wq': 778,\n",
       " 'lar': 779,\n",
       " 'matches': 780,\n",
       " 'polyphonic': 781,\n",
       " 'hl': 782,\n",
       " 'partner': 783,\n",
       " 'simple': 784,\n",
       " 'zed': 785,\n",
       " 'wake': 786,\n",
       " 'lost': 787,\n",
       " 'plus': 788,\n",
       " 'until': 789,\n",
       " 'fa': 790,\n",
       " 'tkts': 791,\n",
       " '87121': 792,\n",
       " 'no1': 793,\n",
       " 'yer': 794,\n",
       " '84199': 795,\n",
       " 'eng': 796,\n",
       " 'box39822': 797,\n",
       " 'w111wx': 798,\n",
       " 'notice': 799,\n",
       " '5000': 800,\n",
       " 'tcs': 801,\n",
       " 'cw25wx': 802,\n",
       " 'na': 803,\n",
       " '1x150p': 804,\n",
       " 'car': 805,\n",
       " 'dad': 806,\n",
       " 'gonna': 807,\n",
       " 'plans': 808,\n",
       " 'little': 809,\n",
       " 'q': 810,\n",
       " 'country': 811,\n",
       " 'ansr': 812,\n",
       " 'tyrone': 813,\n",
       " 'fone': 814,\n",
       " 'write': 815,\n",
       " 'porn': 816,\n",
       " \"i'd\": 817,\n",
       " 'else': 818,\n",
       " 'plan': 819,\n",
       " 'put': 820,\n",
       " 'jordan': 821,\n",
       " '24': 822,\n",
       " 'recd': 823,\n",
       " 'cust': 824,\n",
       " 'old': 825,\n",
       " 'activate': 826,\n",
       " 'head': 827,\n",
       " 'smile': 828,\n",
       " '7pm': 829,\n",
       " '2optout': 830,\n",
       " 'wondering': 831,\n",
       " 'across': 832,\n",
       " 'evening': 833,\n",
       " 'probably': 834,\n",
       " 'nope': 835,\n",
       " 'hows': 836,\n",
       " \"didn't\": 837,\n",
       " '60p': 838,\n",
       " 'selection': 839,\n",
       " 'picked': 840,\n",
       " 'ppm': 841,\n",
       " 'action': 842,\n",
       " 'euro2004': 843,\n",
       " 'bed': 844,\n",
       " 'bank': 845,\n",
       " 'laid': 846,\n",
       " 'largest': 847,\n",
       " 'replying': 848,\n",
       " 'subscriber': 849,\n",
       " 'party': 850,\n",
       " \"doesn't\": 851,\n",
       " 'same': 852,\n",
       " 'extra': 853,\n",
       " 'tho': 854,\n",
       " 'brand': 855,\n",
       " '7250i': 856,\n",
       " 'w1jhl': 857,\n",
       " '0808': 858,\n",
       " '145': 859,\n",
       " '4742': 860,\n",
       " '9am': 861,\n",
       " '11pm': 862,\n",
       " 'stay': 863,\n",
       " 'anything': 864,\n",
       " 'registered': 865,\n",
       " 'receipt': 866,\n",
       " '80062': 867,\n",
       " 'arcade': 868,\n",
       " '434': 869,\n",
       " 'link': 870,\n",
       " 'college': 871,\n",
       " 'xx': 872,\n",
       " 'super': 873,\n",
       " 'check': 874,\n",
       " 'bloomberg': 875,\n",
       " 'dun': 876,\n",
       " 'girls': 877,\n",
       " 'horny': 878,\n",
       " 'til': 879,\n",
       " 'nite': 880,\n",
       " 'sch': 881,\n",
       " '81151': 882,\n",
       " '4t': 883,\n",
       " 'alone': 884,\n",
       " '40gb': 885,\n",
       " 'pod': 886,\n",
       " \"week's\": 887,\n",
       " 'competition': 888,\n",
       " 'wan': 889,\n",
       " 'loan': 890,\n",
       " 'purpose': 891,\n",
       " 'tenants': 892,\n",
       " 'post': 893,\n",
       " 'afternoon': 894,\n",
       " 'birthday': 895,\n",
       " 'mp3': 896,\n",
       " '83355': 897,\n",
       " 'pc': 898,\n",
       " 'listen': 899,\n",
       " 'than': 900,\n",
       " 'sipix': 901,\n",
       " '09061221066': 902,\n",
       " 'fromm': 903,\n",
       " 'ones': 904,\n",
       " 'contract': 905,\n",
       " 'house': 906,\n",
       " 'pix': 907,\n",
       " '8552': 908,\n",
       " 'vodafone': 909,\n",
       " 'fantasy': 910,\n",
       " 'dream': 911,\n",
       " 'team': 912,\n",
       " 'chennai': 913,\n",
       " 'app': 914,\n",
       " 'friday': 915,\n",
       " 'tncs': 916,\n",
       " 'read': 917,\n",
       " 'announcement': 918,\n",
       " 'vip': 919,\n",
       " 'tickets': 920,\n",
       " 'finish': 921,\n",
       " 'sport': 922,\n",
       " 'hand': 923,\n",
       " 'king': 924,\n",
       " 'credits': 925,\n",
       " 'don': 926,\n",
       " 'happen': 927,\n",
       " '88039': 928,\n",
       " 'skilgme': 929,\n",
       " \"he's\": 930,\n",
       " 'tariffs': 931,\n",
       " 'office': 932,\n",
       " 'children': 933,\n",
       " 'lonely': 934,\n",
       " 'anyone': 935,\n",
       " 'inviting': 936,\n",
       " 'paris': 937,\n",
       " 'wana': 938,\n",
       " 'bahamas': 939,\n",
       " '83600': 940,\n",
       " 'sale': 941,\n",
       " 'subscription': 942,\n",
       " '£4': 943,\n",
       " 'calls£1': 944,\n",
       " 'vary': 945,\n",
       " 'dunno': 946,\n",
       " 'juz': 947,\n",
       " '4u': 948,\n",
       " 'w': 949,\n",
       " 'thk': 950,\n",
       " 'tel': 951,\n",
       " 'eerie': 952,\n",
       " 'title': 953,\n",
       " 'set': 954,\n",
       " 'town': 955,\n",
       " 'singles': 956,\n",
       " 'high': 957,\n",
       " 'luck': 958,\n",
       " \"let's\": 959,\n",
       " 'drive': 960,\n",
       " 'moby': 961,\n",
       " '87131': 962,\n",
       " 'jus': 963,\n",
       " 'txtin': 964,\n",
       " '4info': 965,\n",
       " '786': 966,\n",
       " 'unredeemed': 967,\n",
       " '05': 968,\n",
       " 'london': 969,\n",
       " 'busy': 970,\n",
       " '2nite': 971,\n",
       " 'email': 972,\n",
       " 'girlfrnd': 973,\n",
       " 'comuk': 974,\n",
       " 'jamster': 975,\n",
       " 'é': 976,\n",
       " 'knew': 977,\n",
       " 'ref': 978,\n",
       " '250': 979,\n",
       " 'ho': 980,\n",
       " 'santa': 981,\n",
       " 'spook': 982,\n",
       " 'five': 983,\n",
       " 'cinema': 984,\n",
       " '09061209465': 985,\n",
       " 'suprman': 986,\n",
       " 'matrix3': 987,\n",
       " 'starwars3': 988,\n",
       " '2005': 989,\n",
       " '36504': 990,\n",
       " 'optout': 991,\n",
       " 'issues': 992,\n",
       " '08715705022': 993,\n",
       " 'able': 994,\n",
       " 'voicemail': 995,\n",
       " 'ya': 996,\n",
       " 'needs': 997,\n",
       " 'liverpool': 998,\n",
       " 'played': 999,\n",
       " 'original': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4194"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) # Importance of setting a max size of vocabulary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing and padding on training and testing \n",
    "max_len = 50 # Max number of tokens, used with truncating and padding\n",
    "trunc_type = \"post\" # Truncates sequences of tokens that are longer than max_len, post=right side\n",
    "padding_type = \"post\" # Pads AFTER (with post) if sequence is shorter than max_len\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(train_msg)\n",
    "training_padded = pad_sequences (training_sequences, maxlen = max_len,\n",
    "                                 padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_msg)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_len,\n",
    "                               padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training tensor:  (1195, 50)\n",
      "Shape of testing tensor:  (299, 50)\n"
     ]
    }
   ],
   "source": [
    "# Shape of train tensor\n",
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of testing tensor: ', testing_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to convert to torch tensors the data that I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_padded))\n",
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(training_padded).float()\n",
    "X_test = torch.from_numpy(testing_padded).float()\n",
    "Y_train = torch.from_numpy(train_labels).float()\n",
    "Y_test = torch.from_numpy(test_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(Y_train))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len # Number of input neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done\n",
      "Training accuracy is equal to 0.5062761306762695\n",
      "Testing accuracy is equal to 0.5351170301437378\n",
      "Iteration 2 done\n",
      "Training accuracy is equal to 0.5087866187095642\n",
      "Testing accuracy is equal to 0.5384615659713745\n",
      "Iteration 3 done\n",
      "Training accuracy is equal to 0.5305439233779907\n",
      "Testing accuracy is equal to 0.6354514956474304\n",
      "Iteration 4 done\n",
      "Training accuracy is equal to 0.6167364120483398\n",
      "Testing accuracy is equal to 0.7290970087051392\n",
      "Iteration 5 done\n",
      "Training accuracy is equal to 0.7305439114570618\n",
      "Testing accuracy is equal to 0.6387959718704224\n",
      "Iteration 6 done\n",
      "Training accuracy is equal to 0.6861924529075623\n",
      "Testing accuracy is equal to 0.6655518412590027\n",
      "Iteration 7 done\n",
      "Training accuracy is equal to 0.7037656903266907\n",
      "Testing accuracy is equal to 0.7224080562591553\n",
      "Iteration 8 done\n",
      "Training accuracy is equal to 0.73221755027771\n",
      "Testing accuracy is equal to 0.7926421165466309\n",
      "Iteration 9 done\n",
      "Training accuracy is equal to 0.7698744535446167\n",
      "Testing accuracy is equal to 0.8026756048202515\n",
      "Iteration 10 done\n",
      "Training accuracy is equal to 0.7866109013557434\n",
      "Testing accuracy is equal to 0.8026756048202515\n",
      "Iteration 11 done\n",
      "Training accuracy is equal to 0.7933054566383362\n",
      "Testing accuracy is equal to 0.7926421165466309\n",
      "Iteration 12 done\n",
      "Training accuracy is equal to 0.7882845401763916\n",
      "Testing accuracy is equal to 0.7959865927696228\n",
      "Iteration 13 done\n",
      "Training accuracy is equal to 0.7924686074256897\n",
      "Testing accuracy is equal to 0.785953164100647\n",
      "Iteration 14 done\n",
      "Training accuracy is equal to 0.7966527342796326\n",
      "Testing accuracy is equal to 0.785953164100647\n",
      "Iteration 15 done\n",
      "Training accuracy is equal to 0.8033472895622253\n",
      "Testing accuracy is equal to 0.7725752592086792\n",
      "Iteration 16 done\n",
      "Training accuracy is equal to 0.7991631627082825\n",
      "Testing accuracy is equal to 0.7658863067626953\n",
      "Iteration 17 done\n",
      "Training accuracy is equal to 0.8016736507415771\n",
      "Testing accuracy is equal to 0.7558528184890747\n",
      "Iteration 18 done\n",
      "Training accuracy is equal to 0.7941422462463379\n",
      "Testing accuracy is equal to 0.7591972947120667\n",
      "Iteration 19 done\n",
      "Training accuracy is equal to 0.7991631627082825\n",
      "Testing accuracy is equal to 0.7759197354316711\n",
      "Iteration 20 done\n",
      "Training accuracy is equal to 0.8150627613067627\n",
      "Testing accuracy is equal to 0.782608687877655\n",
      "Iteration 21 done\n",
      "Training accuracy is equal to 0.8242678046226501\n",
      "Testing accuracy is equal to 0.7959865927696228\n",
      "Iteration 22 done\n",
      "Training accuracy is equal to 0.8259414434432983\n",
      "Testing accuracy is equal to 0.8026756048202515\n",
      "Iteration 23 done\n",
      "Training accuracy is equal to 0.822594165802002\n",
      "Testing accuracy is equal to 0.8026756048202515\n",
      "Iteration 24 done\n",
      "Training accuracy is equal to 0.822594165802002\n",
      "Testing accuracy is equal to 0.8060200810432434\n",
      "Iteration 25 done\n",
      "Training accuracy is equal to 0.8217573165893555\n",
      "Testing accuracy is equal to 0.7993311285972595\n",
      "Iteration 26 done\n",
      "Training accuracy is equal to 0.8242678046226501\n",
      "Testing accuracy is equal to 0.8026756048202515\n",
      "Iteration 27 done\n",
      "Training accuracy is equal to 0.8317991495132446\n",
      "Testing accuracy is equal to 0.7993311285972595\n",
      "Iteration 28 done\n",
      "Training accuracy is equal to 0.8384937047958374\n",
      "Testing accuracy is equal to 0.8026756048202515\n",
      "Iteration 29 done\n",
      "Training accuracy is equal to 0.8443514704704285\n",
      "Testing accuracy is equal to 0.7959865927696228\n",
      "Iteration 30 done\n",
      "Training accuracy is equal to 0.8393305540084839\n",
      "Testing accuracy is equal to 0.7892976403236389\n"
     ]
    }
   ],
   "source": [
    "iters = 30\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(max_len, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=.01)\n",
    "\n",
    "loss_history = [] #Per epoch\n",
    "accuracy_history = []\n",
    "accuracy_test_history = []\n",
    "\n",
    "for i in range(iters):\n",
    "    proba_pred = net(X_train)  # forward pass\n",
    "    proba_pred = proba_pred.squeeze(-1)  # transform the 1-element vectors into scalars\n",
    "\n",
    "    optimizer.zero_grad() # reset the gradients to 0\n",
    "    loss = criterion(proba_pred, Y_train)\n",
    "    loss_history.append(loss.item()) # .item() to turn it into a python number\n",
    "    loss.backward()  # obtain the gradients with respect to the loss\n",
    "    optimizer.step()  # perform one step of gradient descent\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_pred = proba_pred > 0.5  # Binary label\n",
    "        accuracy = (Y_train == Y_pred).float().mean()\n",
    "        accuracy_history.append(accuracy.item())\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        proba_pred_test = net(X_test)\n",
    "        proba_pred_test = proba_pred_test.squeeze(-1)\n",
    "        Y_pred_test = proba_pred_test > 0.5\n",
    "        accuracy_test = (Y_test == Y_pred_test).float().mean()\n",
    "        accuracy_test_history.append(accuracy_test.item())\n",
    "    \n",
    "    print(\"Iteration {iter} done\".format(iter=i+1))\n",
    "    print(\"Training accuracy is equal to {trainAcc}\".format(trainAcc=accuracy_history[-1]))\n",
    "    print(\"Testing accuracy is equal to {testAcc}\".format(testAcc=accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems to work and we have an instantaneous result.\n",
    "\n",
    "Many things to try, such as :\n",
    "1. Word embedding : the one we use has no reason to be good, we just order words by how often they appear, but it doesn't help us locate the words\n",
    "2. How to handle unbalanced dataset : discarding so many observations is not a great idea, we could also adapt our criterion to evaluate model (instead of just error rate)\n",
    "3. Another architecture for network\n",
    "    1. Convolutional network (why does it work ?)\n",
    "    2. LSTM or any sequential network architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

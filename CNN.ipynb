{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf34ca04",
   "metadata": {},
   "source": [
    "# Convolution neural network\n",
    "Building a CNN on top of the techniques we've described :\n",
    "- Word embedding layer\n",
    "- Upsampling the dataset using back-translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "016fc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Text processing\n",
    "import tensorflow as tf # conda install -c conda-forge tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D #CNN specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01035b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ThinhNguyendai/SMSSpamDetection/main/SMSSpamCollection\" #Use the RAW one\n",
    "messages = pd.read_csv(url, sep ='\\t', names=[\"label\", \"message\"])\n",
    "\n",
    "#Oversampled part\n",
    "url2 = \"https://raw.githubusercontent.com/ThinhNguyendai/SMSSpamDetection/main/Spam\"\n",
    "new_spam = pd.read_csv(url2, sep ='\\t', names=[\"message\"]) # No label like the usual file here\n",
    "spam_labels = [\"spam\" for i in range(len(new_spam))]\n",
    "new_spam.insert(0, \"label\", spam_labels, allow_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43f692d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>spam</td>\n",
       "      <td>You have a secret admirer who is looking 2 con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>spam</td>\n",
       "      <td>25p 4 Alfie Moon's Children in need song on ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>spam</td>\n",
       "      <td>Block Breaker now comes in deluxe format with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>spam</td>\n",
       "      <td>Sun vacation. To claim your medical holiday, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello baby xu 4goten about me?' scammers are g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9650 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2      ham  U dun say so early hor... U c already then say...\n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4      ham  Even my brother is not like to speak with me. ...\n",
       "...    ...                                                ...\n",
       "9645  spam  You have a secret admirer who is looking 2 con...\n",
       "9646  spam  25p 4 Alfie Moon's Children in need song on ur...\n",
       "9647  spam  Block Breaker now comes in deluxe format with ...\n",
       "9648  spam  Sun vacation. To claim your medical holiday, s...\n",
       "9649  spam  Hello baby xu 4goten about me?' scammers are g...\n",
       "\n",
       "[9650 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_msg = messages[messages.label =='ham']\n",
    "spam_msg = messages[messages.label=='spam']\n",
    "new_spam_df = new_spam.sample(n = len(ham_msg) - len(spam_msg), random_state = 754)\n",
    "msg_df = pd.concat([ham_msg, spam_msg, new_spam_df])\n",
    "msg_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d8f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_labels = (msg_df['label'].map({'ham': 0, 'spam': 1})).values\n",
    "train_msg, test_msg, train_labels, test_labels = train_test_split(msg_df['message'],\n",
    "                                                                  msg_labels,\n",
    "                                                                  test_size=0.2,\n",
    "                                                                  random_state=705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "598c135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer : turn words into integers\n",
    "oov_tok = \"<OOV>\" # What to replace words that are not in the vocabulary with\n",
    "vocab_size = 500 # Maximum number of words for tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size,\n",
    "                      char_level=False, # Work words by word\n",
    "                      oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15f98181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8947"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "len(word_index) #Before using data augmentation : had 4194 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aeac48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing and padding on training and testing \n",
    "max_len = 50 # Max number of tokens, used with truncating and padding\n",
    "trunc_type = \"post\" # Truncates sequences of tokens that are longer than max_len, post=right side\n",
    "padding_type = \"post\" # Pads AFTER (with post) if sequence is shorter than max_len\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(train_msg)\n",
    "training_padded = pad_sequences (training_sequences, maxlen = max_len,\n",
    "                                 padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_msg)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_len,\n",
    "                               padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb14bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training array:  (7720, 50)\n",
      "Shape of testing array:  (1930, 50)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training array: ', training_padded.shape)\n",
    "print('Shape of testing array: ', testing_padded.shape)\n",
    "print(type(training_padded))\n",
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "debacef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some reason, sensitivity and specificity are not by default in Keras\n",
    "#Source of this code : https://www.sabinasz.net/unbalanced-classes-machine-learning/\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3f7df",
   "metadata": {},
   "source": [
    "### Keras functions to define neural network\n",
    "Embedding layer takes as input a vector of length *input_length* of **integers**, where the integers are between 0 and *vocab_size-1* (both bounds included). The output is a matrix of dimensions *input_length* X *output_dim*. In other words, each input neuron is projected into a space of dimension *output_dim*.\n",
    "\n",
    "The flatten layer flattens the 2D output into a 1D array\n",
    "\n",
    "Dense layer is another name for the regular fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaf1cb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 32)            16000     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 221,057\n",
      "Trainable params: 221,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#vocab_size = 500\n",
    "#max_len = 50\n",
    "embedding_dim = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', sensitivity, specificity])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35e33c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "309/309 [==============================] - 4s 8ms/step - loss: 0.1745 - accuracy: 0.9291 - sensitivity: 0.9217 - specificity: 0.9390 - val_loss: 0.0843 - val_accuracy: 0.9715 - val_sensitivity: 0.9521 - val_specificity: 0.9894\n",
      "Epoch 2/15\n",
      "309/309 [==============================] - 1s 5ms/step - loss: 0.0420 - accuracy: 0.9853 - sensitivity: 0.9801 - specificity: 0.9908 - val_loss: 0.0515 - val_accuracy: 0.9832 - val_sensitivity: 0.9834 - val_specificity: 0.9819\n",
      "Epoch 3/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0178 - accuracy: 0.9945 - sensitivity: 0.9921 - specificity: 0.9970 - val_loss: 0.0506 - val_accuracy: 0.9858 - val_sensitivity: 0.9893 - val_specificity: 0.9808\n",
      "Epoch 4/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0065 - accuracy: 0.9989 - sensitivity: 0.9987 - specificity: 0.9988 - val_loss: 0.0458 - val_accuracy: 0.9883 - val_sensitivity: 0.9863 - val_specificity: 0.9889\n",
      "Epoch 5/15\n",
      "309/309 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.9995 - sensitivity: 0.9994 - specificity: 0.9997 - val_loss: 0.0502 - val_accuracy: 0.9890 - val_sensitivity: 0.9883 - val_specificity: 0.9877\n",
      "Epoch 6/15\n",
      "309/309 [==============================] - 1s 5ms/step - loss: 0.0024 - accuracy: 0.9997 - sensitivity: 0.9993 - specificity: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9883 - val_sensitivity: 0.9892 - val_specificity: 0.9867\n",
      "Epoch 7/15\n",
      "309/309 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.9997 - sensitivity: 0.9995 - specificity: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9890 - val_sensitivity: 0.9883 - val_specificity: 0.9877\n",
      "Epoch 8/15\n",
      "309/309 [==============================] - 1s 5ms/step - loss: 0.0017 - accuracy: 0.9997 - sensitivity: 0.9997 - specificity: 0.9997 - val_loss: 0.0536 - val_accuracy: 0.9896 - val_sensitivity: 0.9901 - val_specificity: 0.9877\n",
      "Epoch 9/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0017 - accuracy: 0.9995 - sensitivity: 0.9998 - specificity: 0.9993 - val_loss: 0.0544 - val_accuracy: 0.9896 - val_sensitivity: 0.9901 - val_specificity: 0.9877\n",
      "Epoch 10/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0017 - accuracy: 0.9997 - sensitivity: 0.9995 - specificity: 0.9997 - val_loss: 0.0586 - val_accuracy: 0.9877 - val_sensitivity: 0.9872 - val_specificity: 0.9868\n",
      "Epoch 11/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0016 - accuracy: 0.9998 - sensitivity: 0.9997 - specificity: 1.0000 - val_loss: 0.0644 - val_accuracy: 0.9870 - val_sensitivity: 0.9844 - val_specificity: 0.9887\n",
      "Epoch 12/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9998 - sensitivity: 0.9996 - specificity: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9877 - val_sensitivity: 0.9852 - val_specificity: 0.9898\n",
      "Epoch 13/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9997 - sensitivity: 0.9996 - specificity: 0.9997 - val_loss: 0.0679 - val_accuracy: 0.9890 - val_sensitivity: 0.9872 - val_specificity: 0.9898\n",
      "Epoch 14/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.9998 - sensitivity: 0.9996 - specificity: 1.0000 - val_loss: 0.0665 - val_accuracy: 0.9877 - val_sensitivity: 0.9862 - val_specificity: 0.9888\n",
      "Epoch 15/15\n",
      "309/309 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.9997 - sensitivity: 0.9998 - specificity: 0.9997 - val_loss: 0.0716 - val_accuracy: 0.9877 - val_sensitivity: 0.9872 - val_specificity: 0.9875\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(training_padded, train_labels, validation_split=0.2, epochs=15, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88dac9",
   "metadata": {},
   "source": [
    "## Commenting on results\n",
    "This new neural network performs significantly better.\n",
    "It's difficult to say whether it's due to simply using more parameters or the embedding layer, but the results are there.\n",
    "\n",
    "**Might be worth checking whether the choices of hyperparameters make sense**. Max_len and vocab_size might no longer be appropriate for this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8ffac",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "It would be nice to obtain a model with significantly less parameters but similar or even better performance.\n",
    "The size of the model hasn't been an issue in our case, but can quickly become one as we work with more complicated language processing problems.\n",
    "\n",
    "This is a 1D convolutional neural network. **I'm not sure why it would work well for language processing problems, since convolutional neural networks were designed for image processing**. The basic idea of a convolutional layer is that it allows to extract features from the input image, and pooling layers allow you to combine those features. I guess I could vaguely see how that would be useful for text classification, but we'll see.\n",
    "\n",
    "I'm going to work with the simple layers that Keras gives, such as Conv1D layer, MaxPooling1D layer and AveragePooling1D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1419920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embedding_dim = 32\n",
    "batch_size = 20\n",
    "\n",
    "conv1_size = 24 #No ideas bro\n",
    "conv2_size = 32\n",
    "fc_size = 64  # Reduced size of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "462d5315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 50, 32)            16000     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 50, 24)            792       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 48, 32)            2336      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 68,409\n",
      "Trainable params: 68,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN = Sequential()\n",
    "CNN.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "# No flatten because Conv1D input is formatted this way\n",
    "CNN.add(Conv1D(conv1_size, 1, activation='relu'))\n",
    "CNN.add(Conv1D(conv2_size, 3, activation='relu'))\n",
    "CNN.add(MaxPooling1D(pool_size=2))\n",
    "CNN.add(Flatten())\n",
    "CNN.add(Dense(fc_size, activation='relu'))\n",
    "CNN.add(Dense(1, activation='sigmoid'))\n",
    "CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', sensitivity, specificity])\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bc31d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "309/309 [==============================] - 3s 11ms/step - loss: 0.0104 - accuracy: 0.9963 - sensitivity: 0.9959 - specificity: 0.9966 - val_loss: 0.0711 - val_accuracy: 0.9883 - val_sensitivity: 0.9868 - val_specificity: 0.9861\n",
      "Epoch 2/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0086 - accuracy: 0.9968 - sensitivity: 0.9963 - specificity: 0.9973 - val_loss: 0.0822 - val_accuracy: 0.9851 - val_sensitivity: 0.9783 - val_specificity: 0.9873\n",
      "Epoch 3/15\n",
      "309/309 [==============================] - 2s 8ms/step - loss: 0.0094 - accuracy: 0.9977 - sensitivity: 0.9965 - specificity: 0.9987 - val_loss: 0.0653 - val_accuracy: 0.9909 - val_sensitivity: 0.9869 - val_specificity: 0.9910\n",
      "Epoch 4/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0028 - accuracy: 0.9997 - sensitivity: 0.9994 - specificity: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9903 - val_sensitivity: 0.9869 - val_specificity: 0.9892\n",
      "Epoch 5/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0018 - accuracy: 0.9997 - sensitivity: 0.9994 - specificity: 1.0000 - val_loss: 0.0757 - val_accuracy: 0.9896 - val_sensitivity: 0.9883 - val_specificity: 0.9865\n",
      "Epoch 6/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0017 - accuracy: 0.9997 - sensitivity: 0.9995 - specificity: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9890 - val_sensitivity: 0.9897 - val_specificity: 0.9834\n",
      "Epoch 7/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0040 - accuracy: 0.9987 - sensitivity: 0.9983 - specificity: 0.9990 - val_loss: 0.1111 - val_accuracy: 0.9819 - val_sensitivity: 0.9910 - val_specificity: 0.9676\n",
      "Epoch 8/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0038 - accuracy: 0.9989 - sensitivity: 0.9986 - specificity: 0.9991 - val_loss: 0.0932 - val_accuracy: 0.9858 - val_sensitivity: 0.9853 - val_specificity: 0.9838\n",
      "Epoch 9/15\n",
      "309/309 [==============================] - 3s 8ms/step - loss: 0.0048 - accuracy: 0.9985 - sensitivity: 0.9981 - specificity: 0.9990 - val_loss: 0.0852 - val_accuracy: 0.9877 - val_sensitivity: 0.9867 - val_specificity: 0.9868\n",
      "Epoch 10/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0036 - accuracy: 0.9990 - sensitivity: 0.9990 - specificity: 0.9991 - val_loss: 0.1063 - val_accuracy: 0.9858 - val_sensitivity: 0.9907 - val_specificity: 0.9760\n",
      "Epoch 11/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0149 - accuracy: 0.9964 - sensitivity: 0.9964 - specificity: 0.9964 - val_loss: 0.0910 - val_accuracy: 0.9864 - val_sensitivity: 0.9900 - val_specificity: 0.9767\n",
      "Epoch 12/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0072 - accuracy: 0.9982 - sensitivity: 0.9979 - specificity: 0.9981 - val_loss: 0.0671 - val_accuracy: 0.9916 - val_sensitivity: 0.9863 - val_specificity: 0.9953\n",
      "Epoch 13/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0028 - accuracy: 0.9990 - sensitivity: 0.9982 - specificity: 0.9998 - val_loss: 0.1090 - val_accuracy: 0.9838 - val_sensitivity: 0.9919 - val_specificity: 0.9698\n",
      "Epoch 14/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0015 - accuracy: 0.9995 - sensitivity: 0.9993 - specificity: 0.9997 - val_loss: 0.0812 - val_accuracy: 0.9896 - val_sensitivity: 0.9893 - val_specificity: 0.9868\n",
      "Epoch 15/15\n",
      "309/309 [==============================] - 2s 7ms/step - loss: 0.0011 - accuracy: 0.9998 - sensitivity: 0.9997 - specificity: 1.0000 - val_loss: 0.0864 - val_accuracy: 0.9896 - val_sensitivity: 0.9893 - val_specificity: 0.9868\n"
     ]
    }
   ],
   "source": [
    "histCNN = CNN.fit(training_padded, train_labels, validation_split=0.2, epochs=15, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3932d3a",
   "metadata": {},
   "source": [
    "# Results\n",
    "The training is slower at first, not sure why.\n",
    "\n",
    "We obtain pretty similar results, but the number of parameters is way down. Let's try more convolutional and pooling and less fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a398199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_size = 24\n",
    "conv2_size = 32\n",
    "conv3_size = 32 #Second set of (Conv -> Conv) -> Pooling\n",
    "conv4_size = 48\n",
    "fc_size = 32  # Reduced size of this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1de197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 50, 32)            16000     \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 50, 24)            792       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 48, 32)            2336      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 24, 32)            1056      \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 22, 48)            4656      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 11, 48)            0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 528)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                16928     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 41,801\n",
      "Trainable params: 41,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN2 = Sequential()\n",
    "CNN2.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "CNN2.add(Conv1D(conv1_size, 1, activation='relu'))\n",
    "CNN2.add(Conv1D(conv2_size, 3, activation='relu'))\n",
    "CNN2.add(MaxPooling1D(pool_size=2))\n",
    "CNN2.add(Conv1D(conv3_size, 1, activation='relu'))\n",
    "CNN2.add(Conv1D(conv4_size, 3, activation='relu'))\n",
    "CNN2.add(MaxPooling1D(pool_size=2))\n",
    "CNN2.add(Flatten())\n",
    "CNN2.add(Dense(fc_size, activation='relu'))\n",
    "CNN2.add(Dense(1, activation='sigmoid'))\n",
    "CNN2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', sensitivity, specificity])\n",
    "CNN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b1a5856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "309/309 [==============================] - 11s 31ms/step - loss: 0.1844 - accuracy: 0.9182 - sensitivity: 0.9407 - specificity: 0.8962 - val_loss: 0.0883 - val_accuracy: 0.9683 - val_sensitivity: 0.9821 - val_specificity: 0.9547\n",
      "Epoch 2/15\n",
      "309/309 [==============================] - 15s 49ms/step - loss: 0.0542 - accuracy: 0.9814 - sensitivity: 0.9775 - specificity: 0.9856 - val_loss: 0.0552 - val_accuracy: 0.9832 - val_sensitivity: 0.9799 - val_specificity: 0.9860\n",
      "Epoch 3/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0337 - accuracy: 0.9898 - sensitivity: 0.9879 - specificity: 0.9928 - val_loss: 0.0543 - val_accuracy: 0.9845 - val_sensitivity: 0.9793 - val_specificity: 0.9880\n",
      "Epoch 4/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0298 - accuracy: 0.9913 - sensitivity: 0.9892 - specificity: 0.9936 - val_loss: 0.0640 - val_accuracy: 0.9832 - val_sensitivity: 0.9891 - val_specificity: 0.9727\n",
      "Epoch 5/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0197 - accuracy: 0.9950 - sensitivity: 0.9938 - specificity: 0.9962 - val_loss: 0.0784 - val_accuracy: 0.9845 - val_sensitivity: 0.9901 - val_specificity: 0.9738\n",
      "Epoch 6/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0187 - accuracy: 0.9937 - sensitivity: 0.9918 - specificity: 0.9956 - val_loss: 0.0677 - val_accuracy: 0.9838 - val_sensitivity: 0.9767 - val_specificity: 0.9901\n",
      "Epoch 7/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0213 - accuracy: 0.9924 - sensitivity: 0.9903 - specificity: 0.9933 - val_loss: 0.1130 - val_accuracy: 0.9728 - val_sensitivity: 0.9442 - val_specificity: 0.9969\n",
      "Epoch 8/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0142 - accuracy: 0.9963 - sensitivity: 0.9943 - specificity: 0.9981 - val_loss: 0.0599 - val_accuracy: 0.9890 - val_sensitivity: 0.9872 - val_specificity: 0.9897\n",
      "Epoch 9/15\n",
      "309/309 [==============================] - 2s 6ms/step - loss: 0.0149 - accuracy: 0.9951 - sensitivity: 0.9928 - specificity: 0.9968 - val_loss: 0.0676 - val_accuracy: 0.9858 - val_sensitivity: 0.9859 - val_specificity: 0.9843\n",
      "Epoch 10/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0157 - accuracy: 0.9948 - sensitivity: 0.9940 - specificity: 0.9963 - val_loss: 0.0911 - val_accuracy: 0.9832 - val_sensitivity: 0.9850 - val_specificity: 0.9789\n",
      "Epoch 11/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0084 - accuracy: 0.9977 - sensitivity: 0.9962 - specificity: 0.9988 - val_loss: 0.0876 - val_accuracy: 0.9838 - val_sensitivity: 0.9812 - val_specificity: 0.9841\n",
      "Epoch 12/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0068 - accuracy: 0.9977 - sensitivity: 0.9968 - specificity: 0.9985 - val_loss: 0.1079 - val_accuracy: 0.9858 - val_sensitivity: 0.9886 - val_specificity: 0.9811\n",
      "Epoch 13/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0206 - accuracy: 0.9937 - sensitivity: 0.9930 - specificity: 0.9944 - val_loss: 0.0968 - val_accuracy: 0.9819 - val_sensitivity: 0.9680 - val_specificity: 0.9931\n",
      "Epoch 14/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0127 - accuracy: 0.9953 - sensitivity: 0.9949 - specificity: 0.9960 - val_loss: 0.0903 - val_accuracy: 0.9858 - val_sensitivity: 0.9821 - val_specificity: 0.9863\n",
      "Epoch 15/15\n",
      "309/309 [==============================] - 2s 5ms/step - loss: 0.0040 - accuracy: 0.9989 - sensitivity: 0.9983 - specificity: 0.9996 - val_loss: 0.1004 - val_accuracy: 0.9883 - val_sensitivity: 0.9902 - val_specificity: 0.9841\n"
     ]
    }
   ],
   "source": [
    "histCNN2 = CNN2.fit(training_padded, train_labels, validation_split=0.2, epochs=15, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff20e50",
   "metadata": {},
   "source": [
    "# Results of 2nd CNN\n",
    "Even though we have less parameters, the initial epochs are slower to train. I do not know why.\n",
    "\n",
    "The training is less stable than the other 2 models, but the achieved sensitivity and specificity seem better.\n",
    "**I straight up have no idea how to choose architectures, I'm just trying stuff out**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
